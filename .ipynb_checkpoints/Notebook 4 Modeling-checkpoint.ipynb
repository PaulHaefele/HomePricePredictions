{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a57ec171",
   "metadata": {},
   "source": [
    "# House Price Prediction: Advanced Regression Techniques- Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce38dd43",
   "metadata": {},
   "source": [
    "### Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "315c2670",
   "metadata": {},
   "source": [
    "In this data science project, we will tackle the exciting task of predicting home prices based on various features and attributes. The dataset contains valuable information about different homes, including their size, location, number of rooms, amenities, and more. Our primary objective is to develop robust predictive models that can accurately estimate the sale prices of houses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "393c1d07",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 1;\n",
       "                var nbb_unformatted_code = \"import numpy as np\\nimport pandas as pd\\nimport matplotlib.pyplot as plt\\nimport seaborn as sns\\nfrom sklearn.model_selection import train_test_split\\nfrom sklearn.linear_model import LinearRegression\\nfrom sklearn.tree import DecisionTreeRegressor\\nfrom sklearn.ensemble import RandomForestRegressor\\nfrom xgboost import XGBRegressor\\nfrom sklearn.metrics import mean_squared_error, r2_score\\nfrom sklearn.model_selection import GridSearchCV\\n\\n%load_ext nb_black\";\n",
       "                var nbb_formatted_code = \"import numpy as np\\nimport pandas as pd\\nimport matplotlib.pyplot as plt\\nimport seaborn as sns\\nfrom sklearn.model_selection import train_test_split\\nfrom sklearn.linear_model import LinearRegression\\nfrom sklearn.tree import DecisionTreeRegressor\\nfrom sklearn.ensemble import RandomForestRegressor\\nfrom xgboost import XGBRegressor\\nfrom sklearn.metrics import mean_squared_error, r2_score\\nfrom sklearn.model_selection import GridSearchCV\\n\\n%load_ext nb_black\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "%load_ext nb_black"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5c364941",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 2;\n",
       "                var nbb_unformatted_code = \"# Load the datasets\\nX_train_pca = pd.read_csv(\\\"X_train_pca.csv\\\")\\nX_test_pca = pd.read_csv(\\\"X_test_pca.csv\\\")\\ny_train = pd.read_csv(\\\"y_train.csv\\\")\\ny_test = pd.read_csv(\\\"y_test.csv\\\")\\nfinal_eval_pca = pd.read_csv(\\\"final_eval_pca.csv\\\")\";\n",
       "                var nbb_formatted_code = \"# Load the datasets\\nX_train_pca = pd.read_csv(\\\"X_train_pca.csv\\\")\\nX_test_pca = pd.read_csv(\\\"X_test_pca.csv\\\")\\ny_train = pd.read_csv(\\\"y_train.csv\\\")\\ny_test = pd.read_csv(\\\"y_test.csv\\\")\\nfinal_eval_pca = pd.read_csv(\\\"final_eval_pca.csv\\\")\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load the datasets\n",
    "X_train_pca = pd.read_csv(\"X_train_pca.csv\")\n",
    "X_test_pca = pd.read_csv(\"X_test_pca.csv\")\n",
    "y_train = pd.read_csv(\"y_train.csv\")\n",
    "y_test = pd.read_csv(\"y_test.csv\")\n",
    "final_eval_pca = pd.read_csv(\"final_eval_pca.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b1e3106",
   "metadata": {},
   "source": [
    "### Hyperparameter tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6813941",
   "metadata": {},
   "source": [
    "In this section, we will perform hyperparameter tuning for the selected models using GridSearchCV. Hyperparameter tuning helps us find the best combination of hyperparameters for each model, which can lead to improved model performance. For each model, we will define a parameter grid specifying the hyperparameter values we want to search over. GridSearchCV will then exhaustively search through this parameter grid and evaluate the model's performance using cross-validation to find the best hyperparameters.\n",
    "\n",
    "Let's start by tuning the hyperparameters for each of the following models:\n",
    "1. Decision Tree Regression\n",
    "2. Random Forest Regression\n",
    "3. XGBoost Regression\n",
    "\n",
    "After hyperparameter tuning, we will have the best hyperparameters for each model, which we will use for the final modeling step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7b82cc34",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, estimator=DecisionTreeRegressor(),\n",
       "             param_grid={'max_depth': array([ 3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14]),\n",
       "                         'min_samples_leaf': [1, 2, 4],\n",
       "                         'min_samples_split': [2, 5, 10]})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 3;\n",
       "                var nbb_unformatted_code = \"# Decision tree grid search\\n\\nparam_grid = {\\n    \\\"max_depth\\\": np.arange(3, 15),\\n    \\\"min_samples_split\\\": [2, 5, 10],\\n    \\\"min_samples_leaf\\\": [1, 2, 4],\\n}\\n\\ndt = DecisionTreeRegressor()\\ndt_cv = GridSearchCV(dt, param_grid, cv=5)\\ndt_cv.fit(X_train_pca, y_train)\";\n",
       "                var nbb_formatted_code = \"# Decision tree grid search\\n\\nparam_grid = {\\n    \\\"max_depth\\\": np.arange(3, 15),\\n    \\\"min_samples_split\\\": [2, 5, 10],\\n    \\\"min_samples_leaf\\\": [1, 2, 4],\\n}\\n\\ndt = DecisionTreeRegressor()\\ndt_cv = GridSearchCV(dt, param_grid, cv=5)\\ndt_cv.fit(X_train_pca, y_train)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Decision tree grid search\n",
    "\n",
    "param_grid = {\n",
    "    \"max_depth\": np.arange(3, 15),\n",
    "    \"min_samples_split\": [2, 5, 10],\n",
    "    \"min_samples_leaf\": [1, 2, 4],\n",
    "}\n",
    "\n",
    "dt = DecisionTreeRegressor()\n",
    "dt_cv = GridSearchCV(dt, param_grid, cv=5)\n",
    "dt_cv.fit(X_train_pca, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "038336fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Score:0.7789338961363504\n",
      "Best Parameters: {'max_depth': 6, 'min_samples_leaf': 2, 'min_samples_split': 2}\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 4;\n",
       "                var nbb_unformatted_code = \"print(\\\"Best Score:\\\" + str(dt_cv.best_score_))\\nprint(\\\"Best Parameters: \\\" + str(dt_cv.best_params_))\";\n",
       "                var nbb_formatted_code = \"print(\\\"Best Score:\\\" + str(dt_cv.best_score_))\\nprint(\\\"Best Parameters: \\\" + str(dt_cv.best_params_))\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"Best Score:\" + str(dt_cv.best_score_))\n",
    "print(\"Best Parameters: \" + str(dt_cv.best_params_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1fa8b862",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, estimator=RandomForestRegressor(),\n",
       "             param_grid={'max_depth': [3, 6, 9, 12, 15],\n",
       "                         'min_samples_leaf': [1, 2, 4],\n",
       "                         'min_samples_split': [2, 5, 10],\n",
       "                         'n_estimators': array([ 50, 100, 150, 200, 250])})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 5;\n",
       "                var nbb_unformatted_code = \"# Random Forest grid search\\n\\nparam_grid = {\\n    \\\"n_estimators\\\": np.arange(50, 251, 50),\\n    \\\"max_depth\\\": [3, 6, 9, 12, 15],\\n    \\\"min_samples_split\\\": [2, 5, 10],\\n    \\\"min_samples_leaf\\\": [1, 2, 4],\\n}\\n\\nrf = RandomForestRegressor()\\nrf_cv = GridSearchCV(rf, param_grid, cv=5)\\nrf_cv.fit(X_train_pca, y_train.values.ravel())\";\n",
       "                var nbb_formatted_code = \"# Random Forest grid search\\n\\nparam_grid = {\\n    \\\"n_estimators\\\": np.arange(50, 251, 50),\\n    \\\"max_depth\\\": [3, 6, 9, 12, 15],\\n    \\\"min_samples_split\\\": [2, 5, 10],\\n    \\\"min_samples_leaf\\\": [1, 2, 4],\\n}\\n\\nrf = RandomForestRegressor()\\nrf_cv = GridSearchCV(rf, param_grid, cv=5)\\nrf_cv.fit(X_train_pca, y_train.values.ravel())\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Random Forest grid search\n",
    "\n",
    "param_grid = {\n",
    "    \"n_estimators\": np.arange(50, 251, 50),\n",
    "    \"max_depth\": [3, 6, 9, 12, 15],\n",
    "    \"min_samples_split\": [2, 5, 10],\n",
    "    \"min_samples_leaf\": [1, 2, 4],\n",
    "}\n",
    "\n",
    "rf = RandomForestRegressor()\n",
    "rf_cv = GridSearchCV(rf, param_grid, cv=5)\n",
    "rf_cv.fit(X_train_pca, y_train.values.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6515906a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Score:0.8370923325899223\n",
      "Best Parameters: {'max_depth': 9, 'min_samples_leaf': 2, 'min_samples_split': 5, 'n_estimators': 150}\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 6;\n",
       "                var nbb_unformatted_code = \"print(\\\"Best Score:\\\" + str(rf_cv.best_score_))\\nprint(\\\"Best Parameters: \\\" + str(rf_cv.best_params_))\";\n",
       "                var nbb_formatted_code = \"print(\\\"Best Score:\\\" + str(rf_cv.best_score_))\\nprint(\\\"Best Parameters: \\\" + str(rf_cv.best_params_))\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"Best Score:\" + str(rf_cv.best_score_))\n",
    "print(\"Best Parameters: \" + str(rf_cv.best_params_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d81cce01",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5,\n",
       "             estimator=XGBRegressor(base_score=None, booster=None,\n",
       "                                    callbacks=None, colsample_bylevel=None,\n",
       "                                    colsample_bynode=None,\n",
       "                                    colsample_bytree=None,\n",
       "                                    early_stopping_rounds=None,\n",
       "                                    enable_categorical=False, eval_metric=None,\n",
       "                                    feature_types=None, gamma=None, gpu_id=None,\n",
       "                                    grow_policy=None, importance_type=None,\n",
       "                                    interaction_constraints=None,\n",
       "                                    learning_rate=None, m...\n",
       "                                    max_cat_to_onehot=None, max_delta_step=None,\n",
       "                                    max_depth=None, max_leaves=None,\n",
       "                                    min_child_weight=None, missing=nan,\n",
       "                                    monotone_constraints=None, n_estimators=100,\n",
       "                                    n_jobs=None, num_parallel_tree=None,\n",
       "                                    predictor=None, random_state=None, ...),\n",
       "             param_grid={'gamma': [0.5, 1, 1.5, 2, 5],\n",
       "                         'min_child_weight': [1, 5, 10],\n",
       "                         'n_estimators': array([ 50, 100, 150, 200, 250, 300, 350, 400, 450, 500])})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 14;\n",
       "                var nbb_unformatted_code = \"# XGBoost grid search'\\n\\nparam_grid = {\\n    \\\"n_estimators\\\": np.arange(50, 501, 50),\\n    \\\"min_child_weight\\\": [1, 5, 10],\\n    \\\"gamma\\\": [0.5, 1, 1.5, 2, 5],\\n}\\n\\nxgb = XGBRegressor()\\nxgb_cv = GridSearchCV(xgb, param_grid, cv=5)\\nxgb_cv.fit(X_train_pca, y_train)\";\n",
       "                var nbb_formatted_code = \"# XGBoost grid search'\\n\\nparam_grid = {\\n    \\\"n_estimators\\\": np.arange(50, 501, 50),\\n    \\\"min_child_weight\\\": [1, 5, 10],\\n    \\\"gamma\\\": [0.5, 1, 1.5, 2, 5],\\n}\\n\\nxgb = XGBRegressor()\\nxgb_cv = GridSearchCV(xgb, param_grid, cv=5)\\nxgb_cv.fit(X_train_pca, y_train)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# XGBoost grid search'\n",
    "\n",
    "param_grid = {\n",
    "    \"n_estimators\": np.arange(50, 501, 50),\n",
    "    \"min_child_weight\": [1, 5, 10],\n",
    "    \"gamma\": [0.5, 1, 1.5, 2, 5],\n",
    "}\n",
    "\n",
    "xgb = XGBRegressor()\n",
    "xgb_cv = GridSearchCV(xgb, param_grid, cv=5)\n",
    "xgb_cv.fit(X_train_pca, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bc5d2dad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Score:0.8272512103175584\n",
      "Best Parameters: {'gamma': 0.5, 'min_child_weight': 1, 'n_estimators': 50}\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 8;\n",
       "                var nbb_unformatted_code = \"print(\\\"Best Score:\\\" + str(xgb_cv.best_score_))\\nprint(\\\"Best Parameters: \\\" + str(xgb_cv.best_params_))\";\n",
       "                var nbb_formatted_code = \"print(\\\"Best Score:\\\" + str(xgb_cv.best_score_))\\nprint(\\\"Best Parameters: \\\" + str(xgb_cv.best_params_))\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"Best Score:\" + str(xgb_cv.best_score_))\n",
    "print(\"Best Parameters: \" + str(xgb_cv.best_params_))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c1bbe28",
   "metadata": {},
   "source": [
    "### Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71e2fe69",
   "metadata": {},
   "source": [
    "Now that we have obtained the optimal hyperparameters for our Decision Tree, Random Forest, and XGBoost models through hyperparameter tuning, we can proceed to the exciting modeling phase. In this section, we will build and evaluate these models using the best hyperparameters to predict housing prices effectively.\n",
    "\n",
    "We will start by fitting each model to the training data with the respective hyperparameters, and then we will evaluate their performance on the test set using appropriate regression metrics such as Root Mean Squared Error (RMSE) and R-squared (R2). The models' predictions will be compared to the actual target values to assess their accuracy and predictive capabilities.\n",
    "\n",
    "After evaluating each model's performance, we will identify the best-performing model based on the metrics' results. The model with the highest R2 and the lowest RMSE will be chosen as our top candidate for predicting housing prices.\n",
    "\n",
    "Let's dive into the modeling phase and witness the predictive power of these algorithms on our dataset!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83cdfaca",
   "metadata": {},
   "source": [
    "### Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ba7ad7bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root Mean Squared Error (RMSE) for Linear Regression: 36577.16617143117\n",
      "R-squared for Linear Regression: 0.825575986080112\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 9;\n",
       "                var nbb_unformatted_code = \"# Create a linear regression object\\nregr = LinearRegression()\\nmodel = regr.fit(X_train_pca, y_train)\\n\\n# Make predictions on the test set\\ny_pred = model.predict(X_test_pca)\\n\\n# Calculate RMSE\\nrmse = np.sqrt(mean_squared_error(y_test, y_pred))\\nprint(\\\"Root Mean Squared Error (RMSE) for Linear Regression:\\\", rmse)\\n\\n# Calculate R-squared for Linear Regression\\nr2 = r2_score(y_test, y_pred)\\nprint(\\\"R-squared for Linear Regression:\\\", r2)\";\n",
       "                var nbb_formatted_code = \"# Create a linear regression object\\nregr = LinearRegression()\\nmodel = regr.fit(X_train_pca, y_train)\\n\\n# Make predictions on the test set\\ny_pred = model.predict(X_test_pca)\\n\\n# Calculate RMSE\\nrmse = np.sqrt(mean_squared_error(y_test, y_pred))\\nprint(\\\"Root Mean Squared Error (RMSE) for Linear Regression:\\\", rmse)\\n\\n# Calculate R-squared for Linear Regression\\nr2 = r2_score(y_test, y_pred)\\nprint(\\\"R-squared for Linear Regression:\\\", r2)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create a linear regression object\n",
    "regr = LinearRegression()\n",
    "model = regr.fit(X_train_pca, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = model.predict(X_test_pca)\n",
    "\n",
    "# Calculate RMSE\n",
    "rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "print(\"Root Mean Squared Error (RMSE) for Linear Regression:\", rmse)\n",
    "\n",
    "# Calculate R-squared for Linear Regression\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "print(\"R-squared for Linear Regression:\", r2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c01b0cbf",
   "metadata": {},
   "source": [
    "### Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b3612491",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root Mean Squared Error (RMSE) for Decision Tree Regression: 36466.165965387\n",
      "R-squared for Decision Tree Regression: 0.8266330239033199\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 20;\n",
       "                var nbb_unformatted_code = \"# Create a decision tree regressor object\\ndt = DecisionTreeRegressor(max_depth=6, min_samples_leaf=2, min_samples_split=2)\\nmodel_dt = dt.fit(X_train_pca, y_train)\\n\\n# Make predictions on the test set\\ny_pred_dt = model_dt.predict(X_test_pca)\\n\\n# Calculate RMSE\\nrmse = np.sqrt(mean_squared_error(y_test, y_pred_dt))\\nprint(\\\"Root Mean Squared Error (RMSE) for Decision Tree Regression:\\\", rmse)\\n\\n# Calculate R-squared\\nr2 = r2_score(y_test, y_pred_dt)\\nprint(\\\"R-squared for Decision Tree Regression:\\\", r2)\";\n",
       "                var nbb_formatted_code = \"# Create a decision tree regressor object\\ndt = DecisionTreeRegressor(max_depth=6, min_samples_leaf=2, min_samples_split=2)\\nmodel_dt = dt.fit(X_train_pca, y_train)\\n\\n# Make predictions on the test set\\ny_pred_dt = model_dt.predict(X_test_pca)\\n\\n# Calculate RMSE\\nrmse = np.sqrt(mean_squared_error(y_test, y_pred_dt))\\nprint(\\\"Root Mean Squared Error (RMSE) for Decision Tree Regression:\\\", rmse)\\n\\n# Calculate R-squared\\nr2 = r2_score(y_test, y_pred_dt)\\nprint(\\\"R-squared for Decision Tree Regression:\\\", r2)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create a decision tree regressor object\n",
    "dt = DecisionTreeRegressor(max_depth=6, min_samples_leaf=2, min_samples_split=2)\n",
    "model_dt = dt.fit(X_train_pca, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred_dt = model_dt.predict(X_test_pca)\n",
    "\n",
    "# Calculate RMSE\n",
    "rmse = np.sqrt(mean_squared_error(y_test, y_pred_dt))\n",
    "print(\"Root Mean Squared Error (RMSE) for Decision Tree Regression:\", rmse)\n",
    "\n",
    "# Calculate R-squared\n",
    "r2 = r2_score(y_test, y_pred_dt)\n",
    "print(\"R-squared for Decision Tree Regression:\", r2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8e84d50",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fdcb6b07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root Mean Squared Error (RMSE) for Random Forest Regression: 32328.491669653704\n",
      "R-squared for Random Forest Regression: 0.8637435559578188\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 19;\n",
       "                var nbb_unformatted_code = \"# Create a random forest regressor object\\nrf = RandomForestRegressor(\\n    n_estimators=150, max_depth=9, min_samples_split=5, min_samples_leaf=2\\n)\\nmodel_rf = rf.fit(X_train_pca, y_train.values.ravel())\\n\\n# Make predictions on the test set\\ny_pred_rf = model_rf.predict(X_test_pca)\\n\\n# Calculate RMSE\\nrmse = np.sqrt(mean_squared_error(y_test, y_pred_rf))\\nprint(\\\"Root Mean Squared Error (RMSE) for Random Forest Regression:\\\", rmse)\\n\\n# Calculate R-squared\\nr2 = r2_score(y_test, y_pred_rf)\\nprint(\\\"R-squared for Random Forest Regression:\\\", r2)\";\n",
       "                var nbb_formatted_code = \"# Create a random forest regressor object\\nrf = RandomForestRegressor(\\n    n_estimators=150, max_depth=9, min_samples_split=5, min_samples_leaf=2\\n)\\nmodel_rf = rf.fit(X_train_pca, y_train.values.ravel())\\n\\n# Make predictions on the test set\\ny_pred_rf = model_rf.predict(X_test_pca)\\n\\n# Calculate RMSE\\nrmse = np.sqrt(mean_squared_error(y_test, y_pred_rf))\\nprint(\\\"Root Mean Squared Error (RMSE) for Random Forest Regression:\\\", rmse)\\n\\n# Calculate R-squared\\nr2 = r2_score(y_test, y_pred_rf)\\nprint(\\\"R-squared for Random Forest Regression:\\\", r2)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create a random forest regressor object\n",
    "rf = RandomForestRegressor(\n",
    "    n_estimators=150, max_depth=9, min_samples_split=5, min_samples_leaf=2\n",
    ")\n",
    "model_rf = rf.fit(X_train_pca, y_train.values.ravel())\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred_rf = model_rf.predict(X_test_pca)\n",
    "\n",
    "# Calculate RMSE\n",
    "rmse = np.sqrt(mean_squared_error(y_test, y_pred_rf))\n",
    "print(\"Root Mean Squared Error (RMSE) for Random Forest Regression:\", rmse)\n",
    "\n",
    "# Calculate R-squared\n",
    "r2 = r2_score(y_test, y_pred_rf)\n",
    "print(\"R-squared for Random Forest Regression:\", r2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "721ebd88",
   "metadata": {},
   "source": [
    "### XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e99730e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root Mean Squared Error (RMSE) for XGBoost Regression: 34418.5366007608\n",
      "R-squared for XGBoost Regression: 0.8455560259447763\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 15;\n",
       "                var nbb_unformatted_code = \"# Create an XGBoost regressor object with the best hyperparameters from the grid search\\nxgb = XGBRegressor(n_estimators=50, min_child_weight=1, gamma=0.5)\\n\\n# Fit the XGBoost model on the training data\\nmodel_xgb = xgb.fit(X_train_pca, y_train)\\n\\n# Make predictions on the test set\\ny_pred_xgb = model_xgb.predict(X_test_pca)\\n\\n# Calculate RMSE\\nrmse = np.sqrt(mean_squared_error(y_test, y_pred_xgb))\\nprint(\\\"Root Mean Squared Error (RMSE) for XGBoost Regression:\\\", rmse)\\n\\n# Calculate R-squared\\nr2 = r2_score(y_test, y_pred_xgb)\\nprint(\\\"R-squared for XGBoost Regression:\\\", r2)\";\n",
       "                var nbb_formatted_code = \"# Create an XGBoost regressor object with the best hyperparameters from the grid search\\nxgb = XGBRegressor(n_estimators=50, min_child_weight=1, gamma=0.5)\\n\\n# Fit the XGBoost model on the training data\\nmodel_xgb = xgb.fit(X_train_pca, y_train)\\n\\n# Make predictions on the test set\\ny_pred_xgb = model_xgb.predict(X_test_pca)\\n\\n# Calculate RMSE\\nrmse = np.sqrt(mean_squared_error(y_test, y_pred_xgb))\\nprint(\\\"Root Mean Squared Error (RMSE) for XGBoost Regression:\\\", rmse)\\n\\n# Calculate R-squared\\nr2 = r2_score(y_test, y_pred_xgb)\\nprint(\\\"R-squared for XGBoost Regression:\\\", r2)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create an XGBoost regressor object with the best hyperparameters from the grid search\n",
    "xgb = XGBRegressor(n_estimators=50, min_child_weight=1, gamma=0.5)\n",
    "\n",
    "# Fit the XGBoost model on the training data\n",
    "model_xgb = xgb.fit(X_train_pca, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred_xgb = model_xgb.predict(X_test_pca)\n",
    "\n",
    "# Calculate RMSE\n",
    "rmse = np.sqrt(mean_squared_error(y_test, y_pred_xgb))\n",
    "print(\"Root Mean Squared Error (RMSE) for XGBoost Regression:\", rmse)\n",
    "\n",
    "# Calculate R-squared\n",
    "r2 = r2_score(y_test, y_pred_xgb)\n",
    "print(\"R-squared for XGBoost Regression:\", r2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d5ac362",
   "metadata": {},
   "source": [
    "| Model                   | RMSE          | R-squared     | RMSE as % of Mean Price |\n",
    "|-------------------------|---------------|---------------|-------------------------|\n",
    "| Linear Regression       | 36,577.17     | 0.8256        | 20.20%                  |\n",
    "| Decision Tree Regression| 36,466.17     | 0.8266        | 20.13%                  |\n",
    "| Random Forest Regression| 32,328.49     | 0.8637        | 17.87%                  |\n",
    "| XGBoost Regression      | 34,418.54     | 0.8456        | 19.03%                  |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a916e46",
   "metadata": {},
   "source": [
    "# Findings and Model Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71fc23b1",
   "metadata": {},
   "source": [
    "In this analysis, we evaluated four different regression models to predict house sale prices based on various features. The models we explored were Linear Regression, Decision Tree Regression, Random Forest Regression, and XGBoost Regression. Our main evaluation metrics were Root Mean Squared Error (RMSE) and R-squared, which provided insights into prediction accuracy and model fit."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41d04460",
   "metadata": {},
   "source": [
    "### Overall Performance\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d096e0a6",
   "metadata": {},
   "source": [
    "After performing the hyperparameter tuning for each model, we obtained the following results:\n",
    "\n",
    "Linear Regression: RMSE = 36,577.17, R-squared = 0.826 <br>\n",
    "Decision Tree Regression: RMSE = 36,466.17, R-squared = 0.827 <br>\n",
    "Random Forest Regression: RMSE = 32,328.49, R-squared = 0.864 <br>\n",
    "XGBoost Regression: RMSE = 34,418.54, R-squared = 0.846"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75ce8800",
   "metadata": {},
   "source": [
    "### Best Performing Model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4166bfb",
   "metadata": {},
   "source": [
    "Among the evaluated models, Random Forest Regression achieved the lowest RMSE of 32,328.49, indicating its superior predictive accuracy compared to the other models. Its R-squared of 0.864 shows that around 86.4% of the variance in house sale prices can be explained by the features in the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44270c84",
   "metadata": {},
   "source": [
    "### Model Insights\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d962fc3f",
   "metadata": {},
   "source": [
    "Random Forest and XGBoost Regression models outperformed Linear Regression and Decision Tree Regression. These models are known for their ability to capture complex interactions between features and handle non-linear relationships, which contributed to their improved performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee207983",
   "metadata": {},
   "source": [
    "### Feature Importance\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cf08f3f",
   "metadata": {},
   "source": [
    "The Random Forest and XGBoost models provided feature importance insights, highlighting the most significant features in predicting house sale prices. These insights can guide us in understanding the factors that influence property prices the most.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0792626",
   "metadata": {},
   "source": [
    "### Baseline Model Comparison\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2a034b8",
   "metadata": {},
   "source": [
    "We also compared the performance of a baseline Linear Regression model using only the square footage feature. The full-feature Linear Regression outperformed the baseline, indicating that including more relevant features significantly improved the model's predictive power.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a93e2c22",
   "metadata": {},
   "source": [
    "### Potential Improvements\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25f1a9c3",
   "metadata": {},
   "source": [
    "While our models have shown promising results, there are areas for further improvement. We could explore other advanced regression techniques, fine-tune hyperparameters even more, and consider engineering additional features to enhance model performance.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b5f519b",
   "metadata": {},
   "source": [
    "### Final Recommendations\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68007663",
   "metadata": {},
   "source": [
    "Based on our findings, we recommend using the Random Forest Regression model for predicting house sale prices due to its superior accuracy and ability to handle complex relationships. However, it is essential to consider factors such as model interpretability and computational complexity when making the final decision.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddfe9d3d",
   "metadata": {},
   "source": [
    "# Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08e9d129",
   "metadata": {},
   "source": [
    "\n",
    "In conclusion, the Random Forest Regression model has shown promising performance in predicting the sale prices of houses in the dataset. The RMSE (Root Mean Squared Error) of approximately 32,328.49 dollars signifies the average deviation between the model's predictions and the actual sale prices. Considering that the average sale price of homes in the dataset is around $180,921, the RMSE of 32,328.49 translates to approximately 17.87% of the average sale price.\n",
    "\n",
    "In non-technical terms, this means that the model's predictions have an average error of approximately 17.87% when estimating the sale prices of houses in the dataset. Lower RMSE values indicate better model performance, so reducing this value further would enhance the accuracy of the model's predictions and provide more reliable estimates for home prices.\n",
    "\n",
    "As we move forward, it may be worth exploring additional data preprocessing techniques, feature engineering, or experimenting with more complex models to further improve the accuracy of the predictions. Additionally, evaluating the impact of other potential features on the model's performance could help refine the predictions even further. Overall, continuous fine-tuning and refinement of the model would contribute to more precise and reliable estimations of home prices in future analyses."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
